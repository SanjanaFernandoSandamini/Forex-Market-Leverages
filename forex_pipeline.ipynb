{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15qUuG2nwgDK",
        "outputId": "8440e488-ae8e-4ee1-e0f8-848a43abf421"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching and training models for EURUSD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:39:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching and training models for USDJPY\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:39:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching and training models for USDEUR\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [09:39:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:8000\n",
            " * Running on http://172.28.0.12:8000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "from flask import Flask, request, jsonify\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "\n",
        "fastforex_api_key = os.environ.get(\"FASTFOREX_API_KEY\", \"473d528421-7b5949aefb-t0lvdz\") # Use environment variable\n",
        "FASTFOREX_BASE_URL = \"https://api.fastforex.io\"\n",
        "\n",
        "def fetch_forex_data(symbol=\"EURUSD\"):\n",
        "    base_currency = symbol[:3]\n",
        "    target_currency = symbol[3:]\n",
        "    end_date = datetime.now().date() - timedelta(days=1)\n",
        "    start_date = end_date - timedelta(days=13)\n",
        "\n",
        "    all_data = []\n",
        "    current_date = start_date\n",
        "    while current_date <= end_date:\n",
        "        date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "        url = f\"{FASTFOREX_BASE_URL}/historical\"\n",
        "        params = {\"date\": date_str, \"from\": base_currency, \"to\": target_currency, \"api_key\": fastforex_api_key}\n",
        "        response = requests.get(url, params=params)\n",
        "        if response.status_code != 200:\n",
        "            current_date += timedelta(days=1)\n",
        "            continue\n",
        "        json_data = response.json().get(\"results\") # Corrected to access \"results\"\n",
        "        if json_data and target_currency in json_data:\n",
        "            all_data.append({\"t\": date_str, \"close\": json_data[target_currency]})\n",
        "        current_date += timedelta(days=1)\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    df[\"t\"] = pd.to_datetime(df[\"t\"])\n",
        "    df.set_index(\"t\", inplace=True)\n",
        "    df = df.resample('1h').ffill()\n",
        "    return df\n",
        "\n",
        "def compute_rsi(series: pd.Series, period=14):\n",
        "    delta = series.diff()\n",
        "    gain = delta.where(delta > 0, 0.0)\n",
        "    loss = -delta.where(delta < 0, 0.0)\n",
        "    avg_gain = gain.rolling(period).mean()\n",
        "    avg_loss = loss.rolling(period).mean()\n",
        "    rs = avg_gain / avg_loss\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi.fillna(50)\n",
        "\n",
        "def add_features(df):\n",
        "    if df.empty or len(df) < 14:\n",
        "        print(\"Insufficient data for feature engineering.\")\n",
        "        return df\n",
        "    df[\"returns\"] = df[\"close\"].pct_change()\n",
        "    df[\"ma_10\"] = df[\"close\"].rolling(10).mean()\n",
        "    df[\"volatility\"] = df[\"returns\"].rolling(10).std()\n",
        "    df[\"rsi_14\"] = compute_rsi(df[\"close\"])\n",
        "    df.dropna(inplace=True)\n",
        "    df[\"direction\"] = (df[\"returns\"].shift(-1) > 0).astype(int)\n",
        "    return df\n",
        "\n",
        "def classify_risk(volatility):\n",
        "    if volatility > 0.015:\n",
        "        return \"High\"\n",
        "    elif volatility > 0.007:\n",
        "        return \"Medium\"\n",
        "    else:\n",
        "        return \"Low\"\n",
        "\n",
        "def recommend_leverage(confidence, risk):\n",
        "    if confidence > 0.85 and risk == \"Low\":\n",
        "        return 100\n",
        "    elif confidence > 0.7 and risk != \"High\":\n",
        "        return 50\n",
        "    elif confidence > 0.5:\n",
        "        return 20\n",
        "    else:\n",
        "        return 5\n",
        "\n",
        "def train_models(df):\n",
        "    features = [\"returns\", \"ma_10\", \"volatility\", \"rsi_14\"]\n",
        "    X = df[features]\n",
        "    y = df[\"direction\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    pipelines = {\n",
        "        \"logistic_regression\": LogisticRegression(max_iter=1000),\n",
        "        \"random_forest\": RandomForestClassifier(n_estimators=100),\n",
        "        \"xgboost\": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
        "    }\n",
        "    scalers = {}\n",
        "    for name, model in pipelines.items():\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        joblib.dump(model, f\"{name}_model.joblib\")\n",
        "        joblib.dump(scaler, f\"{name}_scaler.joblib\")\n",
        "        scalers[name] = scaler\n",
        "\n",
        "    # LSTM training\n",
        "    mm = MinMaxScaler()\n",
        "    X_norm = mm.fit_transform(X)\n",
        "    seq_len = 10\n",
        "\n",
        "    class ForexDataset(Dataset):\n",
        "        def __init__(self, data, target):\n",
        "            self.X, self.y = [], []\n",
        "            for i in range(len(data) - seq_len):\n",
        "                self.X.append(data[i:i+seq_len])\n",
        "                self.y.append(target[i+seq_len])\n",
        "            self.X = torch.tensor(self.X, dtype=torch.float32)\n",
        "            self.y = torch.tensor(self.y, dtype=torch.long)\n",
        "        def __len__(self):\n",
        "            return len(self.X)\n",
        "        def __getitem__(self, idx):\n",
        "            return self.X[idx], self.y[idx]\n",
        "\n",
        "    class LSTMModel(nn.Module):\n",
        "        def __init__(self, input_size, hidden_size=64):\n",
        "            super().__init__()\n",
        "            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "            self.fc = nn.Linear(hidden_size, 2)\n",
        "        def forward(self, x):\n",
        "            out, _ = self.lstm(x)\n",
        "            return self.fc(out[:, -1])\n",
        "\n",
        "    dataset = ForexDataset(X_norm, y.values)\n",
        "    loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "    model = LSTMModel(len(features))\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    for epoch in range(5):\n",
        "        for xb, yb in loader:\n",
        "            optimizer.zero_grad()\n",
        "            preds = model(xb)\n",
        "            loss = loss_fn(preds, yb)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    torch.save(model.state_dict(), \"lstm_model.pth\")\n",
        "    joblib.dump(mm, \"lstm_scaler.joblib\")\n",
        "\n",
        "\n",
        "# Flask API\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/predict\", methods=[\"POST\"])\n",
        "def predict():\n",
        "    data = request.json\n",
        "    required_features = [\"returns\", \"ma_10\", \"volatility\", \"rsi_14\"]\n",
        "    if not all(k in data for k in required_features):\n",
        "        return jsonify({\"error\": \"Missing required features\"}), 400\n",
        "\n",
        "    X_input = np.array([[data[f] for f in required_features]])\n",
        "    try:\n",
        "        model = joblib.load(\"xgboost_model.joblib\")\n",
        "        scaler = joblib.load(\"xgboost_scaler.joblib\")\n",
        "        X_scaled = scaler.transform(X_input)\n",
        "        prob = model.predict_proba(X_scaled)[0][1]\n",
        "        pred = int(prob > 0.5)\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": f\"Prediction failed: {str(e)}\"}), 500\n",
        "\n",
        "    risk = classify_risk(data[\"volatility\"])\n",
        "    leverage = recommend_leverage(prob, risk)\n",
        "\n",
        "    return jsonify({\n",
        "        \"direction\": \"Up\" if pred else \"Down\",\n",
        "        \"confidence\": round(prob, 4),\n",
        "        \"risk_level\": risk,\n",
        "        \"recommended_leverage\": leverage\n",
        "    })\n",
        "\n",
        "@app.route(\"/health\", methods=[\"GET\"])\n",
        "def health_check():\n",
        "    return jsonify({\"status\": \"running\"}), 200\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        pairs = [\"EURUSD\", \"USDJPY\", \"USDEUR\"]  # Add any pairs you want to support here\n",
        "        for pair in pairs:\n",
        "            print(f\"Fetching and training models for {pair}\")\n",
        "            df_raw = fetch_forex_data(pair)\n",
        "            df_feat = add_features(df_raw)\n",
        "            train_models(df_feat)\n",
        "        app.run(host=\"0.0.0.0\", port=8000)\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")"
      ]
    }
  ]
}