# -*- coding: utf-8 -*-
"""forex_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ajNpwzhpidtBP9beTwgMf2ARHxaj26NI
"""

# forex_pipeline.py
import pandas as pd
import numpy as np
import requests
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split
import joblib
from flask import Flask, request, jsonify
from datetime import datetime, timedelta
import os

# ========================
# CONFIGURATION
# ========================
fastforex_api_key = os.environ.get("FASTFOREX_API_KEY", "473d528421-7b5949aefb-t0lvdz")
FASTFOREX_BASE_URL = "https://api.fastforex.io"
MODEL_DIR = "models"
os.makedirs(MODEL_DIR, exist_ok=True)

# ========================
# DATA FETCHING
# ========================
def fetch_forex_data(symbol="EURUSD"):
    base_currency = symbol[:3]
    target_currency = symbol[3:]
    end_date = datetime.now().date() - timedelta(days=1)
    start_date = end_date - timedelta(days=13)

    all_data = []
    current_date = start_date
    while current_date <= end_date:
        date_str = current_date.strftime("%Y-%m-%d")
        url = f"{FASTFOREX_BASE_URL}/historical"
        params = {"date": date_str, "from": base_currency, "to": target_currency, "api_key": fastforex_api_key}
        response = requests.get(url, params=params)
        if response.status_code != 200:
            current_date += timedelta(days=1)
            continue
        json_data = response.json().get("results")
        if json_data and target_currency in json_data:
            all_data.append({"t": date_str, "close": json_data[target_currency]})
        current_date += timedelta(days=1)

    df = pd.DataFrame(all_data)
    df["t"] = pd.to_datetime(df["t"])
    df.set_index("t", inplace=True)
    df = df.resample('1h').ffill()
    return df

# ========================
# FEATURE ENGINEERING
# ========================
def compute_rsi(series: pd.Series, period=14):
    delta = series.diff()
    gain = delta.where(delta > 0, 0.0)
    loss = -delta.where(delta < 0, 0.0)
    avg_gain = gain.rolling(period).mean()
    avg_loss = loss.rolling(period).mean()
    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi.fillna(50)

def add_features(df):
    if df.empty or len(df) < 14:
        print("Insufficient data for feature engineering.")
        return df
    df["returns"] = df["close"].pct_change()
    df["ma_10"] = df["close"].rolling(10).mean()
    df["volatility"] = df["returns"].rolling(10).std()
    df["rsi_14"] = compute_rsi(df["close"])
    df.dropna(inplace=True)
    df["direction"] = (df["returns"].shift(-1) > 0).astype(int)
    return df

# ========================
# RISK & LEVERAGE
# ========================
def classify_risk(volatility):
    if volatility > 0.015:
        return "High"
    elif volatility > 0.007:
        return "Medium"
    else:
        return "Low"

def recommend_leverage(confidence, risk):
    if confidence > 0.85 and risk == "Low":
        return 100
    elif confidence > 0.7 and risk != "High":
        return 50
    elif confidence > 0.5:
        return 20
    else:
        return 5

# ========================
# MODEL TRAINING
# ========================
def train_models(df):
    features = ["returns", "ma_10", "volatility", "rsi_14"]
    X = df[features]
    y = df["direction"]

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    models = {
        "logistic_regression": LogisticRegression(max_iter=1000),
        "random_forest": RandomForestClassifier(n_estimators=100),
        "xgboost": XGBClassifier(eval_metric='logloss')  # Removed deprecated param
    }

    for name, model in models.items():
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        model.fit(X_train_scaled, y_train)
        joblib.dump(model, f"{MODEL_DIR}/{name}_model.joblib")
        joblib.dump(scaler, f"{MODEL_DIR}/{name}_scaler.joblib")

    # LSTM model
    mm = MinMaxScaler()
    X_norm = mm.fit_transform(X)
    seq_len = 10

    class ForexDataset(Dataset):
        def __init__(self, data, target):
            X_list, y_list = [], []
            for i in range(len(data) - seq_len):
                X_list.append(data[i:i+seq_len])
                y_list.append(target[i+seq_len])
            self.X = torch.tensor(np.array(X_list), dtype=torch.float32)  # Fixed warning
            self.y = torch.tensor(np.array(y_list), dtype=torch.long)
        def __len__(self):
            return len(self.X)
        def __getitem__(self, idx):
            return self.X[idx], self.y[idx]

    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size=64):
            super().__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
            self.fc = nn.Linear(hidden_size, 2)
        def forward(self, x):
            out, _ = self.lstm(x)
            return self.fc(out[:, -1])

    dataset = ForexDataset(X_norm, y.values)
    loader = DataLoader(dataset, batch_size=32, shuffle=True)
    lstm_model = LSTMModel(len(features))
    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=0.001)
    loss_fn = nn.CrossEntropyLoss()

    for epoch in range(5):
        for xb, yb in loader:
            optimizer.zero_grad()
            preds = lstm_model(xb)
            loss = loss_fn(preds, yb)
            loss.backward()
            optimizer.step()

    torch.save(lstm_model.state_dict(), f"{MODEL_DIR}/lstm_model.pth")
    joblib.dump(mm, f"{MODEL_DIR}/lstm_scaler.joblib")

# ========================
# FLASK API
# ========================
app = Flask(__name__)

@app.route("/predict", methods=["POST"])
def predict():
    data = request.json
    required_features = ["returns", "ma_10", "volatility", "rsi_14"]
    if not all(k in data for k in required_features):
        return jsonify({"error": "Missing required features"}), 400

    X_input = np.array([[data[f] for f in required_features]])
    try:
        model = joblib.load(f"{MODEL_DIR}/xgboost_model.joblib")
        scaler = joblib.load(f"{MODEL_DIR}/xgboost_scaler.joblib")
        X_scaled = scaler.transform(X_input)
        prob = model.predict_proba(X_scaled)[0][1]
        pred = int(prob > 0.5)
    except Exception as e:
        return jsonify({"error": f"Prediction failed: {str(e)}"}), 500

    risk = classify_risk(data["volatility"])
    leverage = recommend_leverage(prob, risk)

    return jsonify({
        "direction": "Up" if pred else "Down",
        "confidence": round(prob, 4),
        "risk_level": risk,
        "recommended_leverage": leverage
    })

@app.route("/health", methods=["GET"])
def health_check():
    return jsonify({"status": "running"}), 200

# ========================
# ENTRY POINT
# ========================
if __name__ == "__main__":
    # Train only if models don't exist
    if not os.path.exists(f"{MODEL_DIR}/xgboost_model.joblib"):
        pairs = ["EURUSD"]
        for pair in pairs:
            print(f"Training models for {pair}...")
            df_raw = fetch_forex_data(pair)
            df_feat = add_features(df_raw)
            train_models(df_feat)
    app.run(host="0.0.0.0", port=8000)